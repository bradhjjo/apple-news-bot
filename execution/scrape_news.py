#!/usr/bin/env python3
"""
ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ìŠ¤í¬ë¦½íŠ¸
Directive: directives/collect_apple_news.md
"""

import feedparser
import json
import os
from datetime import datetime, timedelta
from typing import List, Dict
import requests
from bs4 import BeautifulSoup
import time

def fetch_google_news() -> List[Dict]:
    """Google News RSSì—ì„œ ì• í”Œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
    articles = []
    try:
        url = "https://news.google.com/rss/search?q=Apple+OR+AAPL&hl=en-US&gl=US&ceid=US:en"
        feed = feedparser.parse(url)

        for entry in feed.entries[:20]:  # ìµœëŒ€ 20ê°œ
            articles.append({
                'title': entry.title,
                'source': 'Google News',
                'url': entry.link,
                'published': entry.get('published', ''),
                'summary': entry.get('summary', '')
            })
        print(f"âœ“ Google News: {len(articles)} articles")
    except Exception as e:
        print(f"âœ— Google News error: {e}")

    return articles

def fetch_apple_newsroom() -> List[Dict]:
    """Apple Newsroom RSSì—ì„œ ê³µì‹ ë‰´ìŠ¤ ìˆ˜ì§‘"""
    articles = []
    try:
        url = "https://www.apple.com/newsroom/rss-feed.rss"
        feed = feedparser.parse(url)

        for entry in feed.entries[:10]:
            articles.append({
                'title': entry.title,
                'source': 'Apple Newsroom',
                'url': entry.link,
                'published': entry.get('published', ''),
                'summary': entry.get('summary', '')
            })
        print(f"âœ“ Apple Newsroom: {len(articles)} articles")
    except Exception as e:
        print(f"âœ— Apple Newsroom error: {e}")

    return articles

def fetch_tech_news() -> List[Dict]:
    """ì£¼ìš” í…Œí¬ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì—ì„œ RSS ìˆ˜ì§‘"""
    articles = []
    feeds = {
        'MacRumors': 'https://www.macrumors.com/feed/',
        '9to5Mac': 'https://9to5mac.com/feed/',
        'AppleInsider': 'https://appleinsider.com/rss/news/'
    }

    for source, url in feeds.items():
        try:
            feed = feedparser.parse(url)
            for entry in feed.entries[:10]:
                articles.append({
                    'title': entry.title,
                    'source': source,
                    'url': entry.link,
                    'published': entry.get('published', ''),
                    'summary': entry.get('summary', '')
                })
            print(f"âœ“ {source}: {len([a for a in articles if a['source'] == source])} articles")
            time.sleep(1)  # ìš”ì²­ ê°„ ëŒ€ê¸°
        except Exception as e:
            print(f"âœ— {source} error: {e}")

    return articles

def remove_duplicates(articles: List[Dict]) -> List[Dict]:
    """URL ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°"""
    seen_urls = set()
    unique_articles = []

    for article in articles:
        if article['url'] not in seen_urls:
            seen_urls.add(article['url'])
            unique_articles.append(article)

    return unique_articles

def filter_recent(articles: List[Dict], hours: int = 24) -> List[Dict]:
    """ìµœê·¼ Nì‹œê°„ ì´ë‚´ ë‰´ìŠ¤ë§Œ í•„í„°ë§"""
    # RSS í”¼ë“œëŠ” ì¼ë°˜ì ìœ¼ë¡œ ìµœì‹ ìˆœì´ë¯€ë¡œ ìƒìœ„ í•­ëª©ë§Œ ìœ ì§€
    # ì‹¤ì œ ì‹œê°„ íŒŒì‹±ì€ ë³µì¡í•˜ë¯€ë¡œ ìƒìœ„ í•­ëª© ìš°ì„ 
    return articles[:50]  # ìµœëŒ€ 50ê°œ ìœ ì§€

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ Starting Apple news collection...")

    # ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘
    all_articles = []
    all_articles.extend(fetch_google_news())
    all_articles.extend(fetch_apple_newsroom())
    all_articles.extend(fetch_tech_news())

    # ì¤‘ë³µ ì œê±°
    unique_articles = remove_duplicates(all_articles)
    print(f"\nğŸ“Š Total unique articles: {len(unique_articles)}")

    # ìµœê·¼ ë‰´ìŠ¤ë§Œ í•„í„°ë§
    recent_articles = filter_recent(unique_articles)

    # ê²°ê³¼ ì €ì¥
    output_dir = '.tmp'
    os.makedirs(output_dir, exist_ok=True)
    output_file = os.path.join(output_dir, 'news_articles.json')

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(recent_articles, f, ensure_ascii=False, indent=2)

    print(f"âœ… Saved {len(recent_articles)} articles to {output_file}")

    return len(recent_articles) >= 5  # ìµœì†Œ 5ê°œ ì´ìƒ ìˆ˜ì§‘ ì„±ê³µ

if __name__ == '__main__':
    success = main()
    exit(0 if success else 1)
