#!/usr/bin/env python3
"""
ì†Œì…œ ë¯¸ë””ì–´ ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
Directive: directives/collect_social_media.md
"""

import json
import os
import requests
import time
from datetime import datetime, timedelta
from typing import List, Dict


def fetch_reddit_posts() -> List[Dict]:
    """Redditì—ì„œ ì• í”Œ ê´€ë ¨ í¬ìŠ¤íŠ¸ ìˆ˜ì§‘ (read-only, ì¸ì¦ ë¶ˆí•„ìš”)"""
    posts = []
    
    try:
        # Reddit API ì—†ì´ ì›¹ ìŠ¤í¬ë˜í•‘ ë°©ì‹ìœ¼ë¡œ ë³€ê²½ (ë” ì•ˆì •ì )
        import requests
        
        subreddits = ['apple', 'stocks', 'investing']
        keywords = ['apple', 'aapl', 'iphone', 'ipad', 'mac', 'tim cook']
        
        for subreddit_name in subreddits:
            try:
                # Reddit JSON API ì‚¬ìš© (ì¸ì¦ ë¶ˆí•„ìš”)
                url = f"https://www.reddit.com/r/{subreddit_name}/hot.json?limit=25"
                # ë” ë‚˜ì€ User-Agent ì‚¬ìš©
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }
                
                # ì¬ì‹œë„ ë¡œì§
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        response = requests.get(url, headers=headers, timeout=15)
                        if response.status_code == 200:
                            break
                        elif response.status_code == 429:  # Rate limit
                            print(f"âš ï¸  Reddit r/{subreddit_name} rate limited, waiting...")
                            time.sleep(5 * (attempt + 1))
                        else:
                            print(f"âœ— Reddit r/{subreddit_name} returned status {response.status_code}")
                            if attempt < max_retries - 1:
                                time.sleep(2)
                    except requests.exceptions.Timeout:
                        print(f"âš ï¸  Reddit r/{subreddit_name} timeout, retrying...")
                        if attempt < max_retries - 1:
                            time.sleep(2)
                
                if response.status_code != 200:
                    continue
                
                data = response.json()
                
                for post in data['data']['children']:
                    post_data = post['data']
                    title_lower = post_data['title'].lower()
                    
                    # í‚¤ì›Œë“œ í•„í„°ë§
                    if any(keyword in title_lower for keyword in keywords):
                        posts.append({
                            'platform': 'reddit',
                            'title': post_data['title'],
                            'url': f"https://reddit.com{post_data['permalink']}",
                            'score': post_data['score'],
                            'comments': post_data['num_comments'],
                            'created': datetime.fromtimestamp(post_data['created_utc']).isoformat(),
                            'text': post_data.get('selftext', '')[:500]
                        })
                
                print(f"âœ“ Reddit r/{subreddit_name}: {len([p for p in posts if subreddit_name in p['url']])} posts")
                time.sleep(3)  # Reddit API ì†ë„ ì œí•œ ì¤€ìˆ˜ (ë” ê¸¸ê²Œ)
                
            except Exception as e:
                print(f"âœ— Reddit r/{subreddit_name} error: {e}")
        
    except Exception as e:
        print(f"âœ— Reddit API error: {e}")
    
    return posts


def fetch_hackernews() -> List[Dict]:
    """Hacker Newsì—ì„œ ì• í”Œ ê´€ë ¨ í¬ìŠ¤íŠ¸ ìˆ˜ì§‘"""
    posts = []
    
    try:
        # ìµœì‹  ìŠ¤í† ë¦¬ ID ê°€ì ¸ì˜¤ê¸°
        top_stories_url = "https://hacker-news.firebaseio.com/v0/topstories.json"
        response = requests.get(top_stories_url, timeout=10)
        story_ids = response.json()[:100]  # ìƒìœ„ 100ê°œ
        
        keywords = ['apple', 'aapl', 'iphone', 'ipad', 'mac', 'ios']
        
        for story_id in story_ids[:50]:  # ìµœëŒ€ 50ê°œ í™•ì¸
            try:
                story_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
                story_response = requests.get(story_url, timeout=5)
                story = story_response.json()
                
                if story and 'title' in story:
                    title_lower = story['title'].lower()
                    if any(keyword in title_lower for keyword in keywords):
                        posts.append({
                            'platform': 'hackernews',
                            'title': story['title'],
                            'url': story.get('url', f"https://news.ycombinator.com/item?id={story_id}"),
                            'score': story.get('score', 0),
                            'comments': story.get('descendants', 0),
                            'created': datetime.fromtimestamp(story.get('time', 0)).isoformat(),
                            'text': story.get('text', '')[:500]
                        })
            except Exception as e:
                continue  # ê°œë³„ ìŠ¤í† ë¦¬ ì˜¤ë¥˜ëŠ” ìŠ¤í‚µ
        
        print(f"âœ“ Hacker News: {len(posts)} posts")
        
    except Exception as e:
        print(f"âœ— Hacker News error: {e}")
    
    return posts

def filter_and_sort(posts: List[Dict]) -> List[Dict]:
    """ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ ë° í•„í„°ë§"""
    # ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    sorted_posts = sorted(posts, key=lambda x: x['score'], reverse=True)
    
    # 24ì‹œê°„ ì´ë‚´ í¬ìŠ¤íŠ¸ë§Œ (ê°„ë‹¨í•œ í•„í„°ë§)
    # ì‹¤ì œë¡œëŠ” created ì‹œê°„ íŒŒì‹± í•„ìš”í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” ìƒìœ„ í•­ëª© ìœ ì§€
    return sorted_posts[:30]  # ìƒìœ„ 30ê°œ

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ’¬ Starting social media collection...")
    
    # ëª¨ë“  í”Œë«í¼ì—ì„œ í¬ìŠ¤íŠ¸ ìˆ˜ì§‘
    all_posts = []
    
    try:
        reddit_posts = fetch_reddit_posts()
        all_posts.extend(reddit_posts)
    except Exception as e:
        print(f"âš ï¸  Reddit collection failed: {e}")
    
    try:
        hn_posts = fetch_hackernews()
        all_posts.extend(hn_posts)
    except Exception as e:
        print(f"âš ï¸  Hacker News collection failed: {e}")
    
    # ì •ë ¬ ë° í•„í„°ë§
    filtered_posts = filter_and_sort(all_posts)
    print(f"\nğŸ“Š Total filtered posts: {len(filtered_posts)}")
    
    # ê²°ê³¼ ì €ì¥ (ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¼ë„ ì €ì¥)
    output_dir = '.tmp'
    os.makedirs(output_dir, exist_ok=True)
    output_file = os.path.join(output_dir, 'social_posts.json')
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(filtered_posts, f, ensure_ascii=False, indent=2)
    
    if len(filtered_posts) == 0:
        print("âš ï¸  No social media posts collected, but continuing workflow...")
        print(f"âœ… Saved empty posts list to {output_file}")
    else:
        print(f"âœ… Saved {len(filtered_posts)} posts to {output_file}")
    
    # í•­ìƒ ì„±ê³µ ë°˜í™˜ (ì†Œì…œ ë¯¸ë””ì–´ ìˆ˜ì§‘ ì‹¤íŒ¨ê°€ ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ ì¤‘ë‹¨í•˜ì§€ ì•Šë„ë¡)
    return True

if __name__ == '__main__':
    success = main()
    exit(0 if success else 1)
