#!/usr/bin/env python3
"""
ì†Œì…œ ë¯¸ë””ì–´ ìˆ˜ì§‘ ìŠ¤í¬ë¦½íŠ¸
Directive: directives/collect_social_media.md
"""

import json
import os
import requests
import time
from datetime import datetime, timedelta
from typing import List, Dict


def fetch_reddit_rss() -> List[Dict]:
    """Reddit RSS í”¼ë“œë¡œ ì• í”Œ ê´€ë ¨ í¬ìŠ¤íŠ¸ ìˆ˜ì§‘ (ìš°íšŒ ë°©ë²•)"""
    posts = []
    
    try:
        import feedparser
        
        # Reddit RSS í”¼ë“œ ì‚¬ìš© (JSON APIë³´ë‹¤ ì°¨ë‹¨ ê°€ëŠ¥ì„± ë‚®ìŒ)
        subreddits = ['apple', 'stocks', 'investing', 'wallstreetbets']
        keywords = ['apple', 'aapl', 'iphone', 'ipad', 'mac', 'tim cook']
        
        for subreddit_name in subreddits:
            try:
                # Reddit RSS í”¼ë“œ URL (.rss í™•ì¥ì ì‚¬ìš©)
                url = f"https://www.reddit.com/r/{subreddit_name}/hot.rss?limit=50"
                
                # feedparserëŠ” User-Agentë¥¼ ìë™ìœ¼ë¡œ ì„¤ì •
                feed = feedparser.parse(url)
                
                if not feed.entries:
                    print(f"âœ— Reddit r/{subreddit_name} RSS returned no entries")
                    continue
                
                for entry in feed.entries:
                    title_lower = entry.title.lower()
                    
                    # í‚¤ì›Œë“œ í•„í„°ë§
                    if any(keyword in title_lower for keyword in keywords):
                        # RSSì—ì„œ ì ìˆ˜ ì¶”ì¶œ (summaryì— í¬í•¨ë˜ì–´ ìˆìŒ)
                        score = 0
                        comments = 0
                        
                        # summaryì—ì„œ ì ìˆ˜ì™€ ëŒ“ê¸€ ìˆ˜ íŒŒì‹± ì‹œë„
                        if hasattr(entry, 'summary'):
                            import re
                            score_match = re.search(r'(\d+)\s+points?', entry.summary)
                            comments_match = re.search(r'(\d+)\s+comments?', entry.summary)
                            if score_match:
                                score = int(score_match.group(1))
                            if comments_match:
                                comments = int(comments_match.group(1))
                        
                        posts.append({
                            'platform': 'reddit',
                            'title': entry.title,
                            'url': entry.link,
                            'score': score,
                            'comments': comments,
                            'created': entry.get('published', datetime.now().isoformat()),
                            'text': entry.get('summary', '')[:500]
                        })
                
                print(f"âœ“ Reddit r/{subreddit_name} RSS: {len([p for p in posts if subreddit_name in p['url']])} posts")
                time.sleep(2)  # RSS í”¼ë“œë„ ì†ë„ ì œí•œ ì¤€ìˆ˜
                
            except Exception as e:
                print(f"âœ— Reddit r/{subreddit_name} RSS error: {e}")
        
    except Exception as e:
        print(f"âœ— Reddit RSS error: {e}")
    
    return posts


def fetch_google_news_discussions() -> List[Dict]:
    """Google Newsì—ì„œ ì• í”Œ ê´€ë ¨ í† ë¡ /ì˜ê²¬ ê¸°ì‚¬ ìˆ˜ì§‘"""
    posts = []
    
    try:
        import feedparser
        
        # Google News RSS - ì˜ê²¬/ë¶„ì„ ê¸°ì‚¬
        queries = [
            'Apple stock analysis',
            'AAPL stock opinion',
            'Apple earnings discussion'
        ]
        
        for query in queries:
            try:
                url = f"https://news.google.com/rss/search?q={query.replace(' ', '+')}&hl=en-US&gl=US&ceid=US:en"
                feed = feedparser.parse(url)
                
                for entry in feed.entries[:10]:
                    posts.append({
                        'platform': 'google_news',
                        'title': entry.title,
                        'url': entry.link,
                        'score': 0,  # Google News doesn't have scores
                        'comments': 0,
                        'created': entry.get('published', datetime.now().isoformat()),
                        'text': entry.get('summary', '')[:500]
                    })
                
                print(f"âœ“ Google News ({query}): {len([p for p in posts if query.split()[0].lower() in p['title'].lower()])} articles")
                time.sleep(1)
                
            except Exception as e:
                print(f"âœ— Google News ({query}) error: {e}")
        
    except Exception as e:
        print(f"âœ— Google News error: {e}")
    
    return posts


def fetch_seeking_alpha_rss() -> List[Dict]:
    """Seeking Alpha RSSì—ì„œ ì• í”Œ ê´€ë ¨ ë¶„ì„ ìˆ˜ì§‘"""
    posts = []
    
    try:
        import feedparser
        
        # Seeking Alpha Apple í”¼ë“œ
        url = "https://seekingalpha.com/api/sa/combined/AAPL.xml"
        
        try:
            feed = feedparser.parse(url)
            
            for entry in feed.entries[:15]:
                posts.append({
                    'platform': 'seeking_alpha',
                    'title': entry.title,
                    'url': entry.link,
                    'score': 0,
                    'comments': 0,
                    'created': entry.get('published', datetime.now().isoformat()),
                    'text': entry.get('summary', '')[:500]
                })
            
            print(f"âœ“ Seeking Alpha: {len(posts)} articles")
            
        except Exception as e:
            print(f"âœ— Seeking Alpha error: {e}")
        
    except Exception as e:
        print(f"âœ— Seeking Alpha RSS error: {e}")
    
    return posts


def fetch_hackernews() -> List[Dict]:
    """Hacker Newsì—ì„œ ì• í”Œ ê´€ë ¨ í¬ìŠ¤íŠ¸ ìˆ˜ì§‘"""
    posts = []
    
    try:
        # ìµœì‹  ìŠ¤í† ë¦¬ ID ê°€ì ¸ì˜¤ê¸°
        top_stories_url = "https://hacker-news.firebaseio.com/v0/topstories.json"
        response = requests.get(top_stories_url, timeout=10)
        story_ids = response.json()[:100]  # ìƒìœ„ 100ê°œ
        
        keywords = ['apple', 'aapl', 'iphone', 'ipad', 'mac', 'ios']
        
        for story_id in story_ids[:50]:  # ìµœëŒ€ 50ê°œ í™•ì¸
            try:
                story_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
                story_response = requests.get(story_url, timeout=5)
                story = story_response.json()
                
                if story and 'title' in story:
                    title_lower = story['title'].lower()
                    if any(keyword in title_lower for keyword in keywords):
                        posts.append({
                            'platform': 'hackernews',
                            'title': story['title'],
                            'url': story.get('url', f"https://news.ycombinator.com/item?id={story_id}"),
                            'score': story.get('score', 0),
                            'comments': story.get('descendants', 0),
                            'created': datetime.fromtimestamp(story.get('time', 0)).isoformat(),
                            'text': story.get('text', '')[:500]
                        })
            except Exception as e:
                continue  # ê°œë³„ ìŠ¤í† ë¦¬ ì˜¤ë¥˜ëŠ” ìŠ¤í‚µ
        
        print(f"âœ“ Hacker News: {len(posts)} posts")
        
    except Exception as e:
        print(f"âœ— Hacker News error: {e}")
    
    return posts

def filter_and_sort(posts: List[Dict]) -> List[Dict]:
    """ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ ë° í•„í„°ë§"""
    # ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    sorted_posts = sorted(posts, key=lambda x: x['score'], reverse=True)
    
    # 24ì‹œê°„ ì´ë‚´ í¬ìŠ¤íŠ¸ë§Œ (ê°„ë‹¨í•œ í•„í„°ë§)
    # ì‹¤ì œë¡œëŠ” created ì‹œê°„ íŒŒì‹± í•„ìš”í•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” ìƒìœ„ í•­ëª© ìœ ì§€
    return sorted_posts[:30]  # ìƒìœ„ 30ê°œ

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ’¬ Starting social media collection...")
    
    # ëª¨ë“  í”Œë«í¼ì—ì„œ í¬ìŠ¤íŠ¸ ìˆ˜ì§‘
    all_posts = []
    
    # Reddit RSS ì‹œë„ (ìš°íšŒ ë°©ë²•)
    try:
        reddit_posts = fetch_reddit_rss()
        all_posts.extend(reddit_posts)
    except Exception as e:
        print(f"âš ï¸  Reddit RSS collection failed: {e}")
    
    try:
        google_posts = fetch_google_news_discussions()
        all_posts.extend(google_posts)
    except Exception as e:
        print(f"âš ï¸  Google News collection failed: {e}")
    
    try:
        sa_posts = fetch_seeking_alpha_rss()
        all_posts.extend(sa_posts)
    except Exception as e:
        print(f"âš ï¸  Seeking Alpha collection failed: {e}")
    
    try:
        hn_posts = fetch_hackernews()
        all_posts.extend(hn_posts)
    except Exception as e:
        print(f"âš ï¸  Hacker News collection failed: {e}")
    
    # ì •ë ¬ ë° í•„í„°ë§
    filtered_posts = filter_and_sort(all_posts)
    print(f"\nğŸ“Š Total filtered posts: {len(filtered_posts)}")
    
    # ê²°ê³¼ ì €ì¥ (ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¼ë„ ì €ì¥)
    output_dir = '.tmp'
    os.makedirs(output_dir, exist_ok=True)
    output_file = os.path.join(output_dir, 'social_posts.json')
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(filtered_posts, f, ensure_ascii=False, indent=2)
    
    if len(filtered_posts) == 0:
        print("âš ï¸  No social media posts collected, but continuing workflow...")
        print(f"âœ… Saved empty posts list to {output_file}")
    else:
        print(f"âœ… Saved {len(filtered_posts)} posts to {output_file}")
    
    # í•­ìƒ ì„±ê³µ ë°˜í™˜ (ì†Œì…œ ë¯¸ë””ì–´ ìˆ˜ì§‘ ì‹¤íŒ¨ê°€ ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ ì¤‘ë‹¨í•˜ì§€ ì•Šë„ë¡)
    return True

if __name__ == '__main__':
    success = main()
    exit(0 if success else 1)
